{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidh2000/Build-my-ChatGPT/blob/main/Karpathy_GPT_lecture_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jflMx7-Ma260"
      },
      "source": [
        "This notebook is created for the ML4Physics @ Ljubljana school 2025. It follows Andrej Karpathy's GPT lecture [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY), and includes snippets borrowed from Ryan Killian's [notebooks](https://github.com/ryankillian/karpathy-lectures-notebooks) following Karpathy's lecture.\n",
        "\n",
        "We will be using PyTorch. If you have never used PyTorch before, their [tutorials](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#define-the-class) can be helpful. If you feel completely lost, use the hints in the notebook or just ask us to help!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6SXasFttDuO"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8gPUE-1dalyY",
        "outputId": "e60cac93-9389-468c-cfff-693a7448a298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-30 09:03:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-30 09:03:19 (102 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the Tiny Shakespeare text dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S4xkDESua2DG",
        "outputId": "d7af10ee-1912-4d79-d18d-d1d1f6021230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "Vocab size:  65\n",
            "Characters in the dataset:  \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check what's in the dataset\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "chars = sorted(list(set(text)))  # Create a list of characters\n",
        "vocab_size = len(chars)\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"Characters in the dataset: \", \"\".join(chars))\n",
        "print(\"\")\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhH_mOMDtLvq"
      },
      "source": [
        "# Simple tokenizaton, encoding and decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bj7d1pESa2Fo"
      },
      "outputs": [],
      "source": [
        "# Tokenization - simple lookup table\n",
        "char_to_int = {ch:i for i,ch in enumerate(chars)}  # Map charachter to integer\n",
        "int_to_char = {i:ch for i,ch in enumerate(chars)}  # Map integer to character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OOn8etO3a2Ik",
        "outputId": "759b2d60-c745-4969-bd1c-3e20a7c1e522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded string: [46, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoded encoding: hi there\n"
          ]
        }
      ],
      "source": [
        "# Encoding and decoding\n",
        "encode = lambda s: [ char_to_int[c] for c in s]\n",
        "decode = lambda l: \"\".join([ int_to_char[i] for i in l])\n",
        "\n",
        "print(f\"Encoded string: {encode('hi there')}\")\n",
        "print(f\"Decoded encoding: {decode(encode('hi there'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MrYeteUja2Lc",
        "outputId": "bfc2c3b5-606f-4609-e632-86d6182acdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "# Encode the entire text and store as a torch tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CYdaXpdRpFVl",
        "outputId": "ef6eddc7-006f-4b90-e284-348650da9efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEBASTIAN:\n",
            "What, art thou waking?\n",
            "\n",
            "ANTONIO:\n",
            "Do you not hear me speak?\n",
            "\n",
            "SEBASTIAN:\n",
            "I do; and surely\n",
            "It is a sleepy language and thou speak'st\n",
            "Out of thy sleep. What is it thou didst say?\n",
            "This is a strange repose, to be asleep\n",
            "With eyes wide open; standing, speaking, moving,\n",
            "And yet so fast asleep.\n",
            "\n",
            "ANTONIO:\n",
            "Noble Sebastian,\n",
            "Thou let'st thy fortune sleep--die, rather; wink'st\n",
            "Whiles thou art waking.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# How does the story end? Decode the last 401 characters of the text\n",
        "\n",
        "last_401_tokens = data[-401:]  # PyTorch tensor\n",
        "decoded_text = decode(last_401_tokens.tolist())\n",
        "print(decoded_text)\n",
        "# For hint: scroll --->                                                                                                                                                                   use .tolist() to turn the tensor into a format that the decoder can read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd1bd-Pds_I9"
      },
      "source": [
        "# Simple dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9OFKlVl1a2N9",
        "outputId": "d4888075-dbd0-4b65-f779-ec721ca8488d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1003854, 111540)\n"
          ]
        }
      ],
      "source": [
        "# Divide into train/val\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print((len(train_data), len(val_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uI-1yQNczVPM"
      },
      "outputs": [],
      "source": [
        "# Define a maximum context length\n",
        "context_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ek7jlUCba2RZ",
        "outputId": "c349e645-572d-4005-bf53-e350e17dd29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
            "When input is tensor([18]), the target is 47 \n",
            "When input is tensor([18, 47]), the target is 56 \n",
            "When input is tensor([18, 47, 56]), the target is 57 \n",
            "When input is tensor([18, 47, 56, 57]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is 15 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), the target is 47 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]), the target is 64 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]), the target is 52 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]), the target is 10 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]), the target is 14 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]), the target is 44 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]), the target is 53 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1]), the target is 61 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1]), the target is 54 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56]), the target is 53 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]), the target is 41 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]), the target is 42 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39]), the target is 52 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52]), the target is 63 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1]), the target is 44 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44]), the target is 59 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58]), the target is 46 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56]), the target is 6 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1]), the target is 46 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1]), the target is 51 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1]), the target is 57 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57]), the target is 54 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39]), the target is 49 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49]), the target is 8 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0]), the target is 13 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13]), the target is 50 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50]), the target is 50 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50]), the target is 10 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0]), the target is 31 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31]), the target is 54 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39]), the target is 49 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49]), the target is 6 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1]), the target is 57 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57]), the target is 54 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39]), the target is 49 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49]), the target is 8 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0]), the target is 18 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18]), the target is 47 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56]), the target is 57 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1]), the target is 15 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15]), the target is 47 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58]), the target is 47 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47]), the target is 64 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43]), the target is 52 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52]), the target is 10 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10]), the target is 0 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0]), the target is 37 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37]), the target is 53 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53]), the target is 59 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39]), the target is 50 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50]), the target is 50 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43]), the target is 57 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57]), the target is 53 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53]), the target is 50 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50]), the target is 60 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43]), the target is 42 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56]), the target is 39 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58]), the target is 46 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46]), the target is 43 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43]), the target is 56 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1]), the target is 58 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58]), the target is 53 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53]), the target is 1 \n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1]), the target is 42 \n"
          ]
        }
      ],
      "source": [
        "# The target is always the next element in the sequence\n",
        "print(train_data[:10])\n",
        "input = train_data[:context_length]\n",
        "target = train_data[1: context_length+1]\n",
        "for t in range(context_length):\n",
        "    x = input[:t+1]\n",
        "    y = target[t]\n",
        "    print(f\"When input is {x}, the target is {y} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "boj9jpxQrk-V"
      },
      "outputs": [],
      "source": [
        "# Define a batch\n",
        "\n",
        "torch.manual_seed(0) # or pick your favorite number\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split, device='cpu'):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    batch_size = 4\n",
        "\n",
        "    # Random start indices: each must allow a full context window\n",
        "    max_start = len(data) - context_length\n",
        "    ix = torch.randint(0, max_start, (batch_size,))\n",
        "    # Pick batch size (4) random indices between 0 and the length of the data minus the maximum context length\n",
        "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "    inputs = torch.stack([data[i: i+context_length] for i in ix])\n",
        "    targets = torch.stack([data[i+1: i+1+context_length] for i in ix])\n",
        "    # For hint: scroll --->                                                                                                                                                                Target should be the same as inputs, with an offset of 1\n",
        "\n",
        "    return inputs.to(device), targets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CXbfykqwrlPH",
        "outputId": "f2599391-e53d-4fc3-d94c-c5f2a8bc006c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[53, 56, 42, 57,  6,  1, 51, 39, 63,  1, 52, 39, 51, 43,  1, 58, 46, 43,\n",
            "          1, 58, 47, 51, 43, 11,  0, 13, 52, 42,  1, 47, 52,  1, 58, 46, 43,  1,\n",
            "         42, 59, 49, 43,  5, 57,  1, 40, 43, 46, 39, 50, 44,  1, 21,  5, 50, 50,\n",
            "          1, 45, 47, 60, 43,  1, 51, 63,  1, 60, 53, 47, 41, 43,  6,  0, 35, 46,\n",
            "         47, 41, 46,  6,  1, 21,  1, 54, 56, 43, 57, 59, 51, 43,  6,  1, 46, 43,\n",
            "          5, 50, 50,  1, 58, 39, 49, 43,  1, 47, 52,  1, 45, 43, 52, 58, 50, 43,\n",
            "          1, 54, 39, 56, 58,  8,  0,  0, 14, 21, 31, 20, 27, 28,  1, 27, 18,  1,\n",
            "         17, 24],\n",
            "        [43, 11,  1, 44, 53, 56,  1, 21,  1, 42, 47, 42,  1, 49, 47, 50, 50,  1,\n",
            "         23, 47, 52, 45,  1, 20, 43, 52, 56, 63,  6,  0, 14, 59, 58,  1,  5, 58,\n",
            "         61, 39, 57,  1, 58, 46, 63,  1, 40, 43, 39, 59, 58, 63,  1, 58, 46, 39,\n",
            "         58,  1, 54, 56, 53, 60, 53, 49, 43, 42,  1, 51, 43,  8,  0, 26, 39, 63,\n",
            "          6,  1, 52, 53, 61,  1, 42, 47, 57, 54, 39, 58, 41, 46, 11,  1,  5, 58,\n",
            "         61, 39, 57,  1, 21,  1, 58, 46, 39, 58,  1, 57, 58, 39, 40, 40,  5, 42,\n",
            "          1, 63, 53, 59, 52, 45,  1, 17, 42, 61, 39, 56, 42,  6,  0, 14, 59, 58,\n",
            "          1,  5],\n",
            "        [58, 46, 11,  0, 58, 46, 43, 56, 43,  1, 56, 59, 57, 58,  6,  1, 39, 52,\n",
            "         42,  1, 50, 43, 58,  1, 51, 43,  1, 42, 47, 43,  8,  0,  0, 28, 13, 19,\n",
            "         17, 10,  0, 32, 46, 47, 57,  1, 47, 57,  1, 58, 46, 43,  1, 54, 50, 39,\n",
            "         41, 43, 11,  1, 58, 46, 43, 56, 43,  6,  1, 61, 46, 43, 56, 43,  1, 58,\n",
            "         46, 43,  1, 58, 53, 56, 41, 46,  1, 42, 53, 58, 46,  1, 40, 59, 56, 52,\n",
            "          8,  0,  0, 18, 47, 56, 57, 58,  1, 35, 39, 58, 41, 46, 51, 39, 52, 10,\n",
            "          0, 32, 46, 43,  1, 45, 56, 53, 59, 52, 42,  1, 47, 57,  1, 40, 50, 53,\n",
            "         53, 42],\n",
            "        [47, 43, 50, 42, 63,  1, 57, 41, 43, 54, 58, 56, 43,  1, 44, 56, 53, 51,\n",
            "          1, 51, 63,  1, 46, 39, 52, 42,  6,  0, 32, 46, 43,  1, 54, 56, 47, 42,\n",
            "         43,  1, 53, 44,  1, 49, 47, 52, 45, 50, 63,  1, 57, 61, 39, 63,  1, 44,\n",
            "         56, 53, 51,  1, 53, 59, 58,  1, 51, 63,  1, 46, 43, 39, 56, 58, 11,  0,\n",
            "         35, 47, 58, 46,  1, 51, 47, 52, 43,  1, 53, 61, 52,  1, 58, 43, 39, 56,\n",
            "         57,  1, 21,  1, 61, 39, 57, 46,  1, 39, 61, 39, 63,  1, 51, 63,  1, 40,\n",
            "         39, 50, 51,  6,  0, 35, 47, 58, 46,  1, 51, 47, 52, 43,  1, 53, 61, 52,\n",
            "          1, 46]])\n",
            "targets:\n",
            "tensor([[56, 42, 57,  6,  1, 51, 39, 63,  1, 52, 39, 51, 43,  1, 58, 46, 43,  1,\n",
            "         58, 47, 51, 43, 11,  0, 13, 52, 42,  1, 47, 52,  1, 58, 46, 43,  1, 42,\n",
            "         59, 49, 43,  5, 57,  1, 40, 43, 46, 39, 50, 44,  1, 21,  5, 50, 50,  1,\n",
            "         45, 47, 60, 43,  1, 51, 63,  1, 60, 53, 47, 41, 43,  6,  0, 35, 46, 47,\n",
            "         41, 46,  6,  1, 21,  1, 54, 56, 43, 57, 59, 51, 43,  6,  1, 46, 43,  5,\n",
            "         50, 50,  1, 58, 39, 49, 43,  1, 47, 52,  1, 45, 43, 52, 58, 50, 43,  1,\n",
            "         54, 39, 56, 58,  8,  0,  0, 14, 21, 31, 20, 27, 28,  1, 27, 18,  1, 17,\n",
            "         24, 37],\n",
            "        [11,  1, 44, 53, 56,  1, 21,  1, 42, 47, 42,  1, 49, 47, 50, 50,  1, 23,\n",
            "         47, 52, 45,  1, 20, 43, 52, 56, 63,  6,  0, 14, 59, 58,  1,  5, 58, 61,\n",
            "         39, 57,  1, 58, 46, 63,  1, 40, 43, 39, 59, 58, 63,  1, 58, 46, 39, 58,\n",
            "          1, 54, 56, 53, 60, 53, 49, 43, 42,  1, 51, 43,  8,  0, 26, 39, 63,  6,\n",
            "          1, 52, 53, 61,  1, 42, 47, 57, 54, 39, 58, 41, 46, 11,  1,  5, 58, 61,\n",
            "         39, 57,  1, 21,  1, 58, 46, 39, 58,  1, 57, 58, 39, 40, 40,  5, 42,  1,\n",
            "         63, 53, 59, 52, 45,  1, 17, 42, 61, 39, 56, 42,  6,  0, 14, 59, 58,  1,\n",
            "          5, 58],\n",
            "        [46, 11,  0, 58, 46, 43, 56, 43,  1, 56, 59, 57, 58,  6,  1, 39, 52, 42,\n",
            "          1, 50, 43, 58,  1, 51, 43,  1, 42, 47, 43,  8,  0,  0, 28, 13, 19, 17,\n",
            "         10,  0, 32, 46, 47, 57,  1, 47, 57,  1, 58, 46, 43,  1, 54, 50, 39, 41,\n",
            "         43, 11,  1, 58, 46, 43, 56, 43,  6,  1, 61, 46, 43, 56, 43,  1, 58, 46,\n",
            "         43,  1, 58, 53, 56, 41, 46,  1, 42, 53, 58, 46,  1, 40, 59, 56, 52,  8,\n",
            "          0,  0, 18, 47, 56, 57, 58,  1, 35, 39, 58, 41, 46, 51, 39, 52, 10,  0,\n",
            "         32, 46, 43,  1, 45, 56, 53, 59, 52, 42,  1, 47, 57,  1, 40, 50, 53, 53,\n",
            "         42, 63],\n",
            "        [43, 50, 42, 63,  1, 57, 41, 43, 54, 58, 56, 43,  1, 44, 56, 53, 51,  1,\n",
            "         51, 63,  1, 46, 39, 52, 42,  6,  0, 32, 46, 43,  1, 54, 56, 47, 42, 43,\n",
            "          1, 53, 44,  1, 49, 47, 52, 45, 50, 63,  1, 57, 61, 39, 63,  1, 44, 56,\n",
            "         53, 51,  1, 53, 59, 58,  1, 51, 63,  1, 46, 43, 39, 56, 58, 11,  0, 35,\n",
            "         47, 58, 46,  1, 51, 47, 52, 43,  1, 53, 61, 52,  1, 58, 43, 39, 56, 57,\n",
            "          1, 21,  1, 61, 39, 57, 46,  1, 39, 61, 39, 63,  1, 51, 63,  1, 40, 39,\n",
            "         50, 51,  6,  0, 35, 47, 58, 46,  1, 51, 47, 52, 43,  1, 53, 61, 52,  1,\n",
            "         46, 39]])\n"
          ]
        }
      ],
      "source": [
        "# Check that your targets make sense in relation to the inputs\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAt5_SSq5ILP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xm6WQH7m4rY1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRcabfc8Knw"
      },
      "source": [
        "This is the simplest possible model. It contains no connections between tokens, they are not aware of one another. We are only getting back which token typically follows token x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IjCqOlRf4rus"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        # Shape: BTC (batch size, max context length, vocab size)\n",
        "        #print(\"Before\\n\",inputs)\n",
        "        logits = self.token_embedding_table(inputs)\n",
        "        #print(\"After\\n\",logits)\n",
        "\n",
        "        if targets is None: # so we can re-use the function for generation (see below)\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape to fit Pytorch's cross entropy implementation\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, inputs, max_new_tokens):\n",
        "        # inputs is a (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(inputs) # call forward() to get predictions (B, T, C)\n",
        "            logits = logits[:,-1,:] # focus on last element (since we are predicting what comes after the last element), shape becomes (B, C)\n",
        "\n",
        "            # Turn the logits into probabilitites\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            # For hint: scroll --->                                                                                                                                                                                       To get probabilities, we use softmax on the last dimension\n",
        "\n",
        "            # Sample the probabilities to get the predicted index\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # For hint: scroll --->                                                                                                                                                                                       Use torch.multinomial to sample the probability distribution\n",
        "\n",
        "            inputs = torch.cat((inputs, idx_next), dim=1) # concatenate the sampled element, shape is (B, T+1)\n",
        "\n",
        "        return inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V6XAerHV4sFG",
        "outputId": "1931a287-6e6c-40cd-ac0f-64d3bc7e3bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 65])\n",
            "4.563185691833496\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "inputs, targets = get_batch('train')\n",
        "logits, loss = model(inputs, targets)\n",
        "\n",
        "print(logits.shape) # Shape B*T, C\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z_v_ITqI4sbO",
        "outputId": "1d06b473-3bd7-453f-e784-4e9ff99f1492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MRL'Dj-Gae\n",
            "pVEBYbUbEVPlPoJiz,\n",
            "iHgtSTdF'UULLbe.vAHHofsxyYbJ&3iSZ,fLxmqtOBbCl3HgzxNhpkyBiS3tR-Ow-uShxe,G,&\n",
            "K.!:hQOxZLdbir$KiZ!TygzFocdQ-EuZwxXgzOH3Hs:xDqTMsbHFGULx&kmY GvI., T&Gd&&nZk-GdcE$z,CZrroDxSRmAK?\n",
            "muMrBTHg\n",
            "WnETu;w$Pte,NwIkHg? rqNrxyvsnRCAzmyiDh:!&HgVeqGDDnfbsTXxlfrv :Oo!TIqvBxPK:tdTAe !Pv!ahiRU!KxIXSg?R!IZcYtWSZ!'aMHJJdpjAaoONA;w$-PoFM;&-OvyNnpRH!a \n",
            ";UtE'fSSndVULJEh3QOdf!Wqdi$Ko&DWxcQo$wWHYPNm$N\n",
            "Qq3M?DNCsekEFqiO3zFof$wQaTNBTEmZag'bQlx;H'ArrqIeI?;FTYy\n",
            "JJXJdjOG,k:xed\n",
            "IZ:O!-\n",
            "BxVe&?PANOOoBkm$N\n"
          ]
        }
      ],
      "source": [
        "# Generate and decode 500 tokens, starting from a start index\n",
        "start_index = torch.zeros((1,1), dtype=torch.long)\n",
        "generated = model.generate(start_index, max_new_tokens=500)\n",
        "generated_text = decode(generated[0].tolist())  # remove batch dim and decode\n",
        "print(generated_text)\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                   Examine the output of model.generate. Remember how you decoded the output above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1XWagBIM8ly"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf2fMNyPoYmk"
      },
      "source": [
        "The output above is from a randomly initialized model. Let's train it, and see if it learns anything!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sqMHU43orlhB"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5YprmTfApJyD",
        "outputId": "b6429a08-14b3-4140-b7e2-7e1eee69bff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 4.5482096672058105\n",
            "Step 1000, Loss: 3.6333296298980713\n",
            "Step 2000, Loss: 3.0479347705841064\n",
            "Step 3000, Loss: 2.65885066986084\n",
            "Step 4000, Loss: 2.5712459087371826\n",
            "Step 5000, Loss: 2.5849523544311523\n",
            "Step 6000, Loss: 2.4878904819488525\n",
            "Step 7000, Loss: 2.4828920364379883\n",
            "Step 8000, Loss: 2.4297704696655273\n",
            "Step 9000, Loss: 2.4429640769958496\n",
            "Final loss = 2.555318832397461\n"
          ]
        }
      ],
      "source": [
        "# Let's train!\n",
        "batch_size=64\n",
        "\n",
        "for steps in range(10000):\n",
        "    inputs, targets = get_batch('train')  # get batch of shape (B, T)\n",
        "\n",
        "    logits, loss = model(inputs, targets)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 1000 == 0:\n",
        "        print(f\"Step {steps}, Loss: {loss.item()}\")\n",
        "print(f\"Final loss = {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GgO1wgNArlz_",
        "outputId": "d8b7b79e-2711-4215-cb47-acec24c3b949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "th?\n",
            "HA co gonind wind to my KIIUThlalopor, it l warechowhisthe, I wicare on fr ssovey, K:\n",
            "festhemo his k-\n",
            "Taisqur; her-\n",
            "\n",
            "WIOPoincout feso wis ul ck\n",
            "San:\n",
            "RDid athat ond ws sst fuino, ghe? llangey, in\n",
            "nset sthenyo cer.\n",
            "ANTur sare Noreraldsay he, heso, totifour, she ther ghinde,NESe t?\n",
            "To th s 's tl ig ING weth s, tard ik, indigthain t deewo ARDoulde:\n",
            "Codochetr s I bl!\n",
            "RYe die omor trr kias arerorr bof, bara be mint thererthevedelal ay mengoomur we,\n",
            "CEO CEThe ts. mit lind geshy he.\n",
            "\n",
            "BR: Ho.\n",
            "TESithi\n"
          ]
        }
      ],
      "source": [
        "# Generate\n",
        "print(decode(model.generate(inputs=start_index, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mWTvcDNpXtz"
      },
      "source": [
        "Did the output look more reasonable?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get a bit more serious with the training"
      ],
      "metadata": {
        "id": "m1O6NDVd4hRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the rest of this notebook, we would like access to a GPU. If your runtime in Google Colab is on a CPU, you can request a GPU by clicking the little arrow in the top right corner next to the RAM and disk usage graphs, and select \"Change runtime type\". Note that this will restart your runtime, so you need to reload the data etc again. It is possible that Google will say no to your request for a GPU, as they have limited resources. It is possible to run the following also on a CPU, but get a GPU if you can."
      ],
      "metadata": {
        "id": "wNeau5ygbJEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do we have access to a GPU?\n",
        "if torch.cuda.is_available():\n",
        "    print(\"We have access to the following GPU(s):\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(torch.cuda.get_device_properties(i).name)\n",
        "else:\n",
        "    print(\"No access to GPU\")"
      ],
      "metadata": {
        "id": "PjU14PIl1i69",
        "outputId": "8ae02fa2-15d7-49d9-9711-9019af04ad72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have access to the following GPU(s):\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### You only need to run this if you had to restart your runtime at this point\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(0) # or pick your favorite number\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [ char_to_int[c] for c in s]\n",
        "decode = lambda l: \"\".join([ int_to_char[i] for i in l])\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "def get_batch(split, device='cpu'):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "    inputs = torch.stack([data[i: i+context_length] for i in ix])\n",
        "    targets = torch.stack([data[i+1: i+context_length+1] for i in ix])\n",
        "    return inputs.to(device), targets.to(device)"
      ],
      "metadata": {
        "id": "xDxAx4dSb4A3",
        "outputId": "d19065d4-b59f-4cd8-e731-b38f7909b2c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-30 09:03:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2025-06-30 09:03:42 (189 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5pZfquHIrmGP",
        "outputId": "2920a3ca-b21a-4c06-c857-dbb22acef7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5310, val loss 4.5589\n",
            "step 300: train loss 4.1313, val loss 4.1630\n",
            "step 600: train loss 3.7911, val loss 3.8262\n",
            "step 900: train loss 3.5119, val loss 3.5478\n",
            "step 1200: train loss 3.2797, val loss 3.3141\n",
            "step 1500: train loss 3.0947, val loss 3.1262\n",
            "step 1800: train loss 2.9503, val loss 2.9822\n",
            "step 2100: train loss 2.8327, val loss 2.8660\n",
            "step 2400: train loss 2.7458, val loss 2.7756\n",
            "step 2700: train loss 2.6790, val loss 2.7093\n",
            "step 3000: train loss 2.6271, val loss 2.6574\n",
            "step 3300: train loss 2.5911, val loss 2.6167\n",
            "step 3600: train loss 2.5621, val loss 2.5891\n",
            "step 3900: train loss 2.5410, val loss 2.5648\n",
            "step 4200: train loss 2.5256, val loss 2.5498\n",
            "step 4500: train loss 2.5111, val loss 2.5362\n",
            "step 4800: train loss 2.5018, val loss 2.5296\n",
            "step 5100: train loss 2.4956, val loss 2.5186\n",
            "step 5400: train loss 2.4881, val loss 2.5123\n",
            "step 5700: train loss 2.4832, val loss 2.5067\n",
            "step 6000: train loss 2.4807, val loss 2.5063\n",
            "step 6300: train loss 2.4741, val loss 2.4989\n",
            "step 6600: train loss 2.4747, val loss 2.4991\n",
            "step 6900: train loss 2.4695, val loss 2.4934\n",
            "step 7200: train loss 2.4647, val loss 2.4959\n",
            "step 7500: train loss 2.4665, val loss 2.4922\n",
            "step 7800: train loss 2.4629, val loss 2.4926\n",
            "step 8100: train loss 2.4655, val loss 2.4870\n",
            "step 8400: train loss 2.4630, val loss 2.4901\n",
            "step 8700: train loss 2.4594, val loss 2.4902\n",
            "step 9000: train loss 2.4582, val loss 2.4878\n",
            "step 9300: train loss 2.4581, val loss 2.4886\n",
            "step 9600: train loss 2.4591, val loss 2.4889\n",
            "step 9900: train loss 2.4579, val loss 2.4830\n",
            "\n",
            "Pu I heme eald Clloimeabs bind mechithisur ig wen.\n",
            "n;\n",
            "Ag's,$Swant\n",
            "Nurninoure f t ste ithunghe You l tou wim thethesan bur sth t r therean oile, ans f we in, w ave Nakn tathtll d amarid ld\n",
            "\n",
            "\n",
            "\n",
            "Wail be wn vet I'eeck y y mofowevende do omef beld t toowan, s t be eed lle, f he Se hou po m m'schis bif ou meansol d ecaneld thithal felde bus theis, bllmis whey y maina-sthaippewomy hil KI teen.\n",
            "TI pleces ot on coupow--\n",
            "S:\n",
            "ARDUS:\n",
            "\n",
            "PSTha; tr w on indyour a-nedyorlf s ge greaisul gre ve go od thanNOR hathar\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "context_length = 128\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# A function to estimate the loss, which we can call for evaluation\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for data_split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(data_split, device=device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[data_split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOq4AjV_UgHP"
      },
      "source": [
        "# Self-attention, first tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04qWwnNO9KZJ"
      },
      "source": [
        "So far, our model has only had a simple embedding table to work with. Let's introduce some attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "49uSyKLOrmbm"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# The input dimension is (B, T, C), which stands for (batch, time, channels). In\n",
        "# this context, this will be equivalent to (batch size, sequence length,\n",
        "# embedding dimension).\n",
        "\n",
        "# Generate some random input\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfthvGpP-IZF"
      },
      "source": [
        "The formula for attention is:\n",
        "\n",
        "$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "Here, $Q$, $K$ and $V$ will be the output of linear layers (whose input is $x$), and the scaling factor $d_k$ is the dimension of the keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mdAkEtG7GaOg"
      },
      "outputs": [],
      "source": [
        "class SimpleAttentionModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # First, create an embedding table from vocab_size to n_embd\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # For hint: scroll --->                                                                                                                                                                                            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "\n",
        "        # For language, the position matters. Create an embedding table from context_length to n_embd\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "        # For hint: scroll --->                                                                                                                                                                                            self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "\n",
        "\n",
        "        # Set up the Head\n",
        "        self.sa_head = Head(n_embd)\n",
        "\n",
        "        # Set up a projection layer from the embedding dimension back to the vocab_size\n",
        "        self.proj = nn.Linear(n_embd, vocab_size)\n",
        "        # For hint: scroll --->                                                                                                                                                                                            This is just a simple linear layer: self.proj = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.proj(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "\n",
        "            logits, _ = self(idx_cond)  # (B, T, C)\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZyyTZi2d9t9e",
        "outputId": "b98dd8e4-cf4c-4871-d334-415860ddaaa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "# Let's see a single Head perform self-attention\n",
        "head_size = 32\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, hs)\n",
        "q = query(x) # (B, T, hs)\n",
        "v = value(x) # (B, T, hs)\n",
        "\n",
        "\n",
        "# Compute attention scores (\"affinities\") with scaled dot-product\n",
        "wei = q @ k.transpose(-2, -1) / head_size**0.5  # (B, T, T)\n",
        "\n",
        "# Optionally: apply causal mask if this is for autoregressive transformer\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "# Softmax over the keys to get attention weights\n",
        "wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "# Weighted sum of the values\n",
        "attention = wei @ v  # (B, T, head_size)\n",
        "\n",
        "print(attention.shape)  # Should be (B, T, hs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIvXQptka6Vz"
      },
      "source": [
        "# Train a model with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3o5yyyvbJBE"
      },
      "source": [
        "We now add an explicit embedding dimension, `n_embed`. We also introduce a triangular matrix, the causal mask, which will stop the model from cheating by looking ahead at what comes next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zZFzMZSiUmf5"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "context_length = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "head_size = 32\n",
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yluiOcgqa_2c"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # For hint: scroll --->                                                                                                                                                                                       Just as we did before, same setup for Q, K and V: nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "\n",
        "        # New! We introduce a causal mask, that will stop the model from cheating\n",
        "        # by looking ahead at what comes next.\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "\n",
        "        weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        # Apply the causal mask\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
        "        out = weights @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dGNq2x2HfzFh"
      },
      "outputs": [],
      "source": [
        "model = SimpleAttentionModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vlzW489gfztK",
        "outputId": "ffd3bc0b-1fdf-403a-fe0c-a2122d7d4709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2400, val loss 4.2331\n",
            "step 500: train loss 2.6863, val loss 2.6920\n",
            "step 1000: train loss 2.5159, val loss 2.5317\n",
            "step 1500: train loss 2.4719, val loss 2.4637\n",
            "step 2000: train loss 2.4503, val loss 2.4413\n",
            "step 2500: train loss 2.4270, val loss 2.4254\n",
            "step 3000: train loss 2.4040, val loss 2.4155\n",
            "step 3500: train loss 2.3967, val loss 2.4162\n",
            "step 4000: train loss 2.3935, val loss 2.4216\n",
            "step 4500: train loss 2.3871, val loss 2.3882\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jgEREEfxf1xH",
        "outputId": "a684ac0b-9717-40c8-989e-c085a485977a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pur youbn eald Cinoimeabs ut yom chithisur alinen. ncomef br'd ht\n",
            "D yor reak' ht ster oul the You lotoo wim thet hean bur st thor ther an oile, ans fenver ses ave Nikn tath llld am\n",
            "rid ld\n",
            "BE\n",
            "VI:\n",
            "Ase wh vet I'leck yofr.\n",
            "\n",
            "WAdvende do omef beld to\n",
            "Towan, st pradeed lle, fal boer oure mm m'schis bif ous tansol daccanet lour tals ven.\n",
            "\n",
            "BRO.\n",
            "\n",
            "ARIVIINm I whey yomainans nd pamel,\n",
            "I st Ker pounse? pleces ot omy ot bithe hy eghesch le g; tr oron he beer al ady alf onge grein ml gre vergo od thang befre Ef\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8XwTsMhgk-w"
      },
      "source": [
        "# Multihead attention\n",
        "\n",
        "The purpose of multihead attention is to allow different heads to attend differently to the input. Here we are using sequential heads for illustrative purposes. To see an implmentation that utilizes parallel processing (and thus making the most of the GPU), see the [GPT model](https://github.com/uhh-pd-ml/omnijet_alpha/blob/d47046b2bb7f47cea5ee69f616b842fa402758e0/gabbro/models/gpt_model.py#L13) of OmniJet-alpha.\n",
        "\n",
        "Note that `num_heads` times `head_size` must equal `n_embd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6TRL0JRK1t6g"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "context_length = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "num_heads = 4\n",
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B5nKp-NFjUDr",
        "outputId": "b1d7a50a-8fa2-46c0-9c76-ab096f473d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SimpleMHA_Model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-2518744243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# your code here...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# For hint: scroll --->                                                                                                                                                                                           Same as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleMHA_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleMHA_Model' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize the model and send to the correct device\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before, but use the new model\n",
        "\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before\n",
        "model = SimpleMHA_Model()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB5_xnsFjUGO"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before\n",
        "\n",
        "# Train\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0KF3xLmjUIq"
      },
      "outputs": [],
      "source": [
        "# generate from the model\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxsUwaX4kItd"
      },
      "source": [
        "# Adding FF network, residual connections, layer norm and dropout\n",
        "\n",
        "To make our model more powerful, let's add some more stuff to make it more similar to what's in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper. You can experiment with the dropout value, and where you want to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BUVM8A7mUm45"
      },
      "outputs": [],
      "source": [
        "# Let's try multihead attention\n",
        "\n",
        "class Head(nn.Module):\n",
        "    # your code here...\n",
        "    # For hint: scroll --->                                                                                                                                                                                           Same as before\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "\n",
        "        weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        # Apply the causal mask\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
        "        out = weights @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # Heads in sequence\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The output of the heads are concatenated\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out\n",
        "\n",
        "class SimpleMHA_Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Implement token embedding table and position embedding table as earlier\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "\n",
        "\n",
        "        # Note that num_heads x head_size = n_embd\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 8 dimensional self-attention\n",
        "\n",
        "        # Implement the projection layer as earlier\n",
        "        self.proj = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.sa_heads (x)\n",
        "        logits = self.proj(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "\n",
        "            logits, _ = self(idx_cond)  # (B, T, C)\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mLdn0y_RUnR_"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "context_length = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_head = 4\n",
        "n_embd = 32\n",
        "dropout = 0.0\n",
        "\n",
        "                                                                                                                                                                                           # In forward, after softmax: weights = self.dropout(weights)\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "\n",
        "        weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        # Apply the causal mask\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
        "        weights = self.dropout(weights)\n",
        "        out = weights @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # Heads in sequence\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # INItialised dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The output of the heads are concatenated\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        ### ADded dropout\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.FF = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),  # experiment with using/not using dropout here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.FF(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Full_Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "\n",
        "        self.proj = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.proj(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "\n",
        "            logits, _ = self(idx_cond)  # (B, T, C)\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UbYY9UQ2sAyo"
      },
      "outputs": [],
      "source": [
        "# Initialize the model and send to the correct device\n",
        "# your code here...\n",
        "\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "# your code here...\n",
        "\n",
        "# Initialize the model and send to the correct device\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before, but use the new model\n",
        "\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "# your code here...\n",
        "# For hint: scroll --->                                                                                                                                                                                           Same as before\n",
        "model = Full_Model()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "K0tCKGVnsA0-",
        "outputId": "79586750-12c6-42c7-83f7-d125a367361c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5580, val loss 4.5610\n",
            "step 500: train loss 2.4104, val loss 2.4101\n",
            "step 1000: train loss 2.2708, val loss 2.2930\n",
            "step 1500: train loss 2.1876, val loss 2.2168\n",
            "step 2000: train loss 2.1430, val loss 2.1763\n",
            "step 2500: train loss 2.1211, val loss 2.1566\n",
            "step 3000: train loss 2.0791, val loss 2.1389\n",
            "step 3500: train loss 2.0550, val loss 2.1229\n",
            "step 4000: train loss 2.0235, val loss 2.1101\n",
            "step 4500: train loss 2.0257, val loss 2.0992\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "M3L2ICx6sA3w",
        "outputId": "5939930a-7bef-4545-a289-43f7f136fc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Themwal your lial him! I buet wellse it\n",
            "And truade be to no forted:\n",
            "Tome; fartell, I:\n",
            "And sorrtooadle of me, selemele your frew that will younds!\n",
            "sel, cand the corst dod but shope wee,\n",
            "And thim: what I seecontitame ap of of earsimves oaken? Cumbern,\n",
            "Nor sutee 'on ste: in trees.\n",
            "No, Hausparlatt peam,\n",
            "II shall appwion the to Ko just muss:\n",
            "The demess!\n",
            "Sted Rithts lovend. 'Et did nat traced his nawn\n",
            "Wan\n",
            "That hean to lat'ly end oblo, wheasife, seve lanta he is we a my heed.\n",
            "\n",
            "USTIELILAM:\n",
            "WeC\n",
            "I marre s\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6rOjtf13qKV"
      },
      "source": [
        "# Scaling up\n",
        "\n",
        "Foundation models become powerful because of scale. This is not yet a foundation model, just a small transformer, but we still want to check what happens if we scale it up! Note that this will likely take way too long to finish during the school (during my test run on a T4 GPU it took 1.5h to finish 1500 steps). If you have time at home, and perhaps access to more powerful GPUs via your university/institute, play around with the different options and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "l6fhXbqQUnsN"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64  # Larger batch size\n",
        "context_length = 256  # Longer context length\n",
        "max_iters = 5000\n",
        "eval_interval = 10  # <-- Since this model takes a long time to train, I put a short interval here so you won't have to wait forever to see the output\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384  # Larger embedding dimension\n",
        "n_head = 6  # More heads\n",
        "n_blocks = 3  # More transformer blocks\n",
        "dropout = 0.2\n",
        "\n",
        "# A function to estimate the loss, which we can call for evaluation\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for data_split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(data_split, device=device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[data_split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_gXUhdjpaRtK"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "\n",
        "        weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T)\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)  # <- Dropout after softmax\n",
        "        out = weights @ v  # (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)  # ← add this\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))  # ← and use it here\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.FF = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)  # <- Dropout at the end of FFN\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.FF(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))  # residual connection + attention\n",
        "        x = x + self.ffwd(self.ln2(x))  # residual connection + FFN\n",
        "        return x\n",
        "\n",
        "\n",
        "class Full_Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_blocks)]) # many blocks!\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.proj = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.ln_f(self.blocks(x))\n",
        "        logits = self.proj(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "\n",
        "            logits, _ = self(idx_cond)  # (B, T, C)\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TeTINLe0aSUi",
        "outputId": "e525dde5-d504-4651-fb77-5621f9f6bc8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure:\n",
            "Full_Model(\n",
            "  (token_embedding_table): Embedding(65, 384)\n",
            "  (position_embedding_table): Embedding(256, 384)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (FF): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (FF): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (FF): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "  (proj): Linear(in_features=384, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "5.468993 M parameters\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model and send to the correct device\n",
        "model = Full_Model()\n",
        "m = model.to(device)\n",
        "print(f\"Model structure:\\n{model}\\n\")\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0npQTr2QaTMK",
        "outputId": "b3fc182c-d0d8-42b8-fcc5-d1730abdae3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3187, val loss 4.3076\n",
            "step 10: train loss 3.0200, val loss 3.0515\n",
            "step 20: train loss 2.8026, val loss 2.8194\n",
            "step 30: train loss 2.6769, val loss 2.6926\n",
            "step 40: train loss 2.5999, val loss 2.6043\n",
            "step 50: train loss 2.5553, val loss 2.5581\n",
            "step 60: train loss 2.5319, val loss 2.5370\n",
            "step 70: train loss 2.5165, val loss 2.5239\n",
            "step 80: train loss 2.5016, val loss 2.5104\n",
            "step 90: train loss 2.4916, val loss 2.4996\n",
            "step 100: train loss 2.4848, val loss 2.4946\n",
            "step 110: train loss 2.4783, val loss 2.4902\n",
            "step 120: train loss 2.4722, val loss 2.4865\n",
            "step 130: train loss 2.4649, val loss 2.4848\n",
            "step 140: train loss 2.4581, val loss 2.4782\n",
            "step 150: train loss 2.4523, val loss 2.4739\n",
            "step 160: train loss 2.4467, val loss 2.4653\n",
            "step 170: train loss 2.4406, val loss 2.4650\n",
            "step 180: train loss 2.4321, val loss 2.4585\n",
            "step 190: train loss 2.4301, val loss 2.4491\n",
            "step 200: train loss 2.4193, val loss 2.4416\n",
            "step 210: train loss 2.4153, val loss 2.4393\n",
            "step 220: train loss 2.4145, val loss 2.4339\n",
            "step 230: train loss 2.4056, val loss 2.4272\n",
            "step 240: train loss 2.3953, val loss 2.4137\n",
            "step 250: train loss 2.3822, val loss 2.4118\n",
            "step 260: train loss 2.3746, val loss 2.3993\n",
            "step 270: train loss 2.3657, val loss 2.4004\n",
            "step 280: train loss 2.3498, val loss 2.3751\n",
            "step 290: train loss 2.3395, val loss 2.3754\n",
            "step 300: train loss 2.3273, val loss 2.3539\n",
            "step 310: train loss 2.3111, val loss 2.3443\n",
            "step 320: train loss 2.2969, val loss 2.3285\n",
            "step 330: train loss 2.2845, val loss 2.3168\n",
            "step 340: train loss 2.2687, val loss 2.3046\n",
            "step 350: train loss 2.2531, val loss 2.2882\n",
            "step 360: train loss 2.2390, val loss 2.2813\n",
            "step 370: train loss 2.2181, val loss 2.2579\n",
            "step 380: train loss 2.2034, val loss 2.2442\n",
            "step 390: train loss 2.1830, val loss 2.2311\n",
            "step 400: train loss 2.1667, val loss 2.2150\n",
            "step 410: train loss 2.1457, val loss 2.1989\n",
            "step 420: train loss 2.1271, val loss 2.1820\n",
            "step 430: train loss 2.1129, val loss 2.1689\n",
            "step 440: train loss 2.0955, val loss 2.1545\n",
            "step 450: train loss 2.0821, val loss 2.1388\n",
            "step 460: train loss 2.0682, val loss 2.1326\n",
            "step 470: train loss 2.0552, val loss 2.1229\n",
            "step 480: train loss 2.0409, val loss 2.1080\n",
            "step 490: train loss 2.0286, val loss 2.1014\n",
            "step 500: train loss 2.0154, val loss 2.0860\n",
            "step 510: train loss 2.0048, val loss 2.0788\n",
            "step 520: train loss 1.9916, val loss 2.0700\n",
            "step 530: train loss 1.9812, val loss 2.0622\n",
            "step 540: train loss 1.9705, val loss 2.0546\n",
            "step 550: train loss 1.9576, val loss 2.0498\n",
            "step 560: train loss 1.9461, val loss 2.0312\n",
            "step 570: train loss 1.9382, val loss 2.0322\n",
            "step 580: train loss 1.9286, val loss 2.0276\n",
            "step 590: train loss 1.9182, val loss 2.0173\n",
            "step 600: train loss 1.9096, val loss 2.0066\n",
            "step 610: train loss 1.8966, val loss 1.9962\n",
            "step 620: train loss 1.8932, val loss 1.9992\n",
            "step 630: train loss 1.8784, val loss 1.9892\n",
            "step 640: train loss 1.8669, val loss 1.9814\n",
            "step 650: train loss 1.8629, val loss 1.9816\n",
            "step 660: train loss 1.8553, val loss 1.9781\n",
            "step 670: train loss 1.8489, val loss 1.9686\n",
            "step 680: train loss 1.8373, val loss 1.9623\n",
            "step 690: train loss 1.8277, val loss 1.9551\n",
            "step 700: train loss 1.8171, val loss 1.9412\n",
            "step 710: train loss 1.8088, val loss 1.9353\n",
            "step 720: train loss 1.8064, val loss 1.9402\n",
            "step 730: train loss 1.7970, val loss 1.9320\n",
            "step 740: train loss 1.7873, val loss 1.9215\n",
            "step 750: train loss 1.7813, val loss 1.9148\n",
            "step 760: train loss 1.7713, val loss 1.9123\n",
            "step 770: train loss 1.7661, val loss 1.9078\n",
            "step 780: train loss 1.7568, val loss 1.8934\n",
            "step 790: train loss 1.7590, val loss 1.8989\n",
            "step 800: train loss 1.7505, val loss 1.8900\n",
            "step 810: train loss 1.7361, val loss 1.8803\n",
            "step 820: train loss 1.7359, val loss 1.8778\n",
            "step 830: train loss 1.7316, val loss 1.8767\n",
            "step 840: train loss 1.7227, val loss 1.8638\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', device=device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlcVBBwPaVam"
      },
      "outputs": [],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoReVXrv72NJ"
      },
      "source": [
        "I'm done, now what?\n",
        "\n",
        "Congrats! Now you hopefully have a better understanding of what goes on behind the scenes in a GPT model. If you want to continue working with language models, check out Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master). If you want to try a particle physics implementation, check out [OmniJet-alpha](https://github.com/uhh-pd-ml/omnijet_alpha). Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnr4jgmu8W6p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}